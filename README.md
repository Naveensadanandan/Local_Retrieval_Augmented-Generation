# Local_Retrieval_Augmented-Generation

# NutriChat: Local Retrieval Augmented Generation System for Nutrition Textbooks

**NutriChat** is an advanced Local Retrieval Augmented Generation (RAG) system designed to interact with a  Nutrition Textbook. This project leverages  natural language processing (NLP) and LLMs to provide accurate and contextually relevant responses to user queries based on the textbook's content.

## Project Overview

The goal of NutriChat is to enable users to query a comprehensive nutrition textbook and receive detailed, accurate answers generated by a language model. The system processes the textbook text, embeds it into numerical representations, and retrieves relevant information based on the user's query. The response is then generated using a state-of-the-art language model.

## Key Features

- **Text Processing & Embedding:** Utilizes Spacy and NLTK to preprocess and format the textbook text. The text chunks are then embedded using the `sentence-transformers/all-mpnet-base-v2` model for numerical representation.
- **Semantic Search:** Implements a vector search system to identify and retrieve relevant chunks of text based on the user's query, ensuring precise and relevant information retrieval.
- **Response Generation:** Constructs prompts from the retrieved text and generates responses using the Google Gemma-2B-IT language model, providing contextually accurate answers.
- **Scalable Storage:** Features an efficient storage system for managing embeddings, allowing for quick retrieval and response generation from large-scale documents.

Local RAG pipeline we're going to build:

The flowchart illustrates a Retrieval-Augmented Generation (RAG) workflow executed on a Colab GPU for document processing and embedding creation, followed by search and answer generation. The workflow starts with a collection of documents, such as PDFs or large textbooks, which are split into smaller chunks (e.g., 10 sentences each). These segments act as context for the Large Language Model (LLM).

A user may ask a question like "What are macronutrients?" This query is converted into a numerical representation using an embedding model (e.g., Hugging Faceâ€™s sentence transformers), which is stored in a torch.tensor format for efficiency, especially for handling large amounts of embeddings. In cases of extremely large datasets, a vector database or index can be used.

The numerical query and relevant document chunks are processed on a Colab GPU, and the LLM generates an output based on the context retrieved. The generated output can then be interacted with via an optional chat web app.

All the document processing, embedding, and response generation happen on a GPU provided by Google Colab.

In our specific example, we'll build NutriChat, a RAG workflow that allows a person to query a 1200 page PDF version of a Nutrition Textbook and have an LLM generate responses back to the query based on passages of text from the textbook.

PDF source: https://pressbooks.oer.hawaii.edu/humannutrition2/ 

You can also run notebook `00-simple-local-rag.ipynb` directly in [Google Colab](https://github.com/Naveensadanandan/Local_Retrieval_Augmented-Generation/blob/main/Local_Retrieval_Augmented_Generation_RAG.ipynb). 

